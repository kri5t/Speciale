Artificial neural networks are to some extent being used as the foundation for a prediction support system \cite{shim2002past}. In \cite{shim2002past} they present how a decision support system is created in three steps. First analyze the data and the problem at hand. Find a suitable solution for the problem and at last present the results in a digestible format to the client. In our analysis and experiments we have accounted for the first two steps in creating a decision support system. The last step requires a whole survey on its own and are out of the scope of this thesis. Our goal in this section is to clarify what preconditions must be met before an artificial neural network can be used for decision making and what challenges this introduces. A neural network conducts an analysis of a specific dataset and returns a result based on these parameters. The dataset used for a prediction can be both comprehensive and complex. Therefore a decision based on an ANN can only be as informed as the underlying dataset. A thorough understanding of both what kind of preprocessing has been applied to the dataset and where the data originates from are required to gain trust in the system. Another factor is the transparency of how the decision was reached by the system. This calls for transparency and contradicts the black-box nature of artificial neural networks \cite{fromBlackBoxToTransparentBox}. In this section we will elaborate on these concerns and propose solutions to how this can be obtained.

\subsection{Trust in the system - the underlying data}
The underlying data of a prediction is what drives the artificial neural network. It is the core of the prediction and without it we have no prediction. This points out why trusting the dataset is important for the user to trust the predictions presented by the system. The underlying data should always be shown together with the output so the user gets an idea of what inputs resulted in what output. The system must communicate the input parameters and their origin at all times. Knowing where things come from and exactly what input parameters constitute a result is the basis for creating trust. As introduced in~\ref{sec:uncertainInformation} autonomous systems do not always guarantee the best result and the underlying data are important for the best possible result to be obtained. Users might have different preferred sources for the underlying data. Accommodating these needs and building different versions of the system on top of the data from different suppliers will help the system to gain the users trust.

Furthermore we 

\todo{discuss in relation to other texts. Many texts we did not know how to trust because we had no idea of the underlying data and what they used and how they manipulated}

\section{Obtaining transparency - Opening the black-box}
We have obtained the best possible prediction through data manipulation and black box optimization which have been verified through our experiments in Chapter~\ref{ch:experimentalResults}. The network outputs a prediction which in itself could be used as decision support by the traders. The problematic is for the users to know when it can be trusted and why to even trust it. People are not in the habit of blindly trusting everything that is placed in front of them and especially not when it comes to high-risk decisions e.g. trading stocks, buying electricity etc.

Artificial Neural Networks are known to be a black box \cite{fromBlackBoxToTransparentBox} because the only knowledge consist in the input and output but nothing about the internal logic. There is a need for making the black box more transparent by communicating what is happening inside it. This is a field of research related to ANNs where the researchers attempt to open up the artificial neural network to give the user a result that also communicates how the decision was reached. This is called grey-box and refers to different methods that seek to communicate how the result of an ANN was reached \cite{young2010using}. One of the methods is called decomposition and seeks to map the interconnections, weights and structures of the neural network to give the user a sense of trust and insight in how the result was conducted. Another approach is pedagogical where the inner workings of the neural networks are ignored but instead tries to map the input to the output data thus explaining the relationship between the two. The last method eclectic is a hybrid between the two but is not commonly implemented in practice. We do not want to distinguish between the methods and point out the best one - since we have limited insight in these methods. We just want to emphasize the need for transparency in how the decisions was reached in ANNs and how these methods can help the user understand the inner workings.

\subsection{Presenting uncertain information}
The need for presenting uncertain information is presented in~\ref{sec:uncertainInformation}.
f.x. the difference in accuracy when in the lowest and highest numbers.

\subsection{Preprocessing}

\subsection{How to make DSS a reality!?!?!?!}
\begin{itemize}
\item Improve understanding of the underlying data and the output;
\item Transparency is the goal;
\item Let the user take decisions based on the uncertain information;
\item Visualize errors;
\end{itemize}

\todo{the success of the predictions in the dss rely heavily on the use.. users are important!}
\todo{... videregive information og dele. Vores bedste resultat er ikke gaeldende alle steder jvf. 24 timer ahead fra forskellige starting points. Det underliggende bliver noedt til at vaere gjort klart for den, der bruger systemet}