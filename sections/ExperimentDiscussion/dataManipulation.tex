Dataset manipulation and trimming of the dataset are two important techniques that can improve the predictions of an artificial neural network.
\subsection{Matrix}
We used a matrix representation (described in section~\ref{sec:Matrix}) for the input parameters that were applicable for such a representation. The input (applicable for matrix) has to have a finite and limited set of values and a significant difference in the influence on the output between the values. The inspiration for the matrix representation came from the way pictures are represented in ANNs. In \cite{knerr1992handwritten} they take every pixel (which has 16 different shades of grey) in a 16 pixel big image and map the pixels out as a matrix. This gives them 256 input variables for the neural network to represent this image. We do the same thing for the applicable input parameters but instead of pixels we map out values.

The matrix format has limitations (as we saw in the wind power experiment two~\ref{sec:windProdExperimentTwo}). The input parameters you want to represent as a matrix has to be evenly distributed in the dataset e.g. time of day, day of week etc. If this is not the case; some of the input values might be under trained(from under representation) and lead to errors in the predictions instead of improvements. This was the case with wind speed as a matrix and the reason to why it was worse than the wind speed as a single input variable. When we applied the matrix format to the time of day input variable we saw an improvement in both cases. We did a simple matrix test in the wind speed experiment two~\ref{sec:windProdExperimentTwo} as we only had two different parameters that were applicable for the matrix format and because we saw that the wind speed as a matrix did perform poorly. In the price experiments we did a more elaborate test of the matrix format and did every combination (all with matrix, all without matrix and mixed). We did those because the price experiments had more parameters that was representable on the matrix format thus giving us more combinations. The results from the price experiment was not as lopsided regarding the matrix format as the wind production experiments. In the price experiments we could not say that one parameter had to be on one specific format but the combinations of the inputs both as matrix and non-matrix proved to be the best - we saw that the matrix format was overrepresented in the top results giving us a hint about it being the better choice.

We have only seen one other \cite{crowley2005weather} use the matrix format (to some extend) for their seasonal inputs. They represented their days in the week as a matrix format. They do not argue why they did it like that but we can see that they did it in their table of inputs. At the same time they do not evaluate the results compared to non-matrix representation. The following articles has seasonal inputs but do not utilize the matrix format \cite{szkuta1999electricity, singhal2011electricity} or at least do not disseminate it. If they actually do represent their seasonal inputs as matrices they should write it. This follows our thoughts about transparency and reproducibility we argued for in \ref{sec:unseenDataDiscussion}.

\subsection{Trimming}
Trimming of the dataset is a technique to remove irregularities in the dataset. This can be done both for the highest values and the lowest values in the dataset. Trimming allows us to get a more standardized dataset but it comes at a cost. All the data that you remove from the dataset you will not be able to predict in the test dataset thus introducing limitations to the predictions. Therefore trimming should always be weighed up against how much of the dataset you will not be able to predict after the trimming. If the irregularities are extreme enough and under-represented in the dataset then we will not be able to predict these values (because of under-training towards these extremes). If this is the case the outliers just introduce errors and do not add benefits thus justifying the removal of these values.

In the wind power predictions experiment two~\ref{sec:windProdExperimentTwo} there was no benefit from trimming the dataset. Actually it introduced errors and made the predictions worse in average. This is because the dataset used for wind power production already consists of uniform dataset and the trimming will only scramble this data and introduce further volatility (described in~\ref{sec:windProdExperimentTwo} and shown in figure~\ref{fig:fivePercentTrimPrediction}) because of this we will only focus on price in this section. In the electricity price prediction experiment two~\ref{sec:priceExperimentTwo} we saw that the trimming helped the predictions to be better. This is because in the price prediction dataset there were some extreme outliers. When we trimmed the dataset by 1\% in the top we went from 1561 to 632 as the highest value in the dataset. The 1\% we removed from the dataset only introduced more noise and was not predictable thus removing them did not make the performance worse - instead it helped the remainder of the results see figure~\ref{fig:1PTrim}. We saw an improvement of 20,99\% from a non-trimmed dataset compared to a trimmed dataset (in the price predictions).

In \cite{singhal2011electricity, yamin2004adaptive} they also trim the dataset when predicting the electricity prices to get rid of the worst spikes in the price. In \cite{yamin2004adaptive} describe detailed how they use trimming in their dataset. They use a form of trimming called Winsorising\cite{Winsorising} where they instead of removing the data(trimming) they just set the values of the outliers to a specific value. In their case they set the upper value to \$50 and set everything above that to \$50. The interesting part is that they introduce a post-processing scheme where they revert the Winsorising after the predictions thus not removing the high values from the set but just limiting the negative effects of outliers. If this was done on our dataset we would see a decrease in performance\cite{yamin2004adaptive} but we would have a more accurate measure for how it would perform on live data. We see this as future work to test if this method would work on our dataset. Both matrix and trimming shows the need for preprocessing of the dataset also stated in \cite{yamin2004adaptive}. Preprocessing is important but it is also very important to communicate it to the reader of the paper due to the significant improvements in the performance. In \cite{sansom1999neural, 1} they do not mention anything about spike prices or whether they account for it. This might be because their dataset does not contain any spikes which (especially in large datasets) seems implausible see section about price volatility \ref{sec:volatility}. Which means we would have a hard time recreating their experiments and see how well it performs on our dataset.

Another obstacle when talking about trimming is how it can be applied on a live dataset. Since we do not know what the price is for the next 24 hours we will not be able to include this in our calculations regarding what should be trimmed and what should not. If we chose to apply trimming we can only do those calculations on the training set. As mentioned earlier this makes us unable to reproduce the values that has been removed from the dataset. If the system was to be used in a real live setting as a decision support system the aforementioned limitations and what is written in \ref{sec:uncertainInformation} really emphasizes how important transparency is. We need to make the user aware of the limitations and benefits of trimming thus giving them an informed choice to make.

\todo{Skriv om den her i forhold til pjmForecast}