This section describes what needs to consider when building and modeling the network in terms of actual design and data set manipulation.

\section{Neural Network Optimization}
\label{sec:neuralNetworkOptimization}
The forecasting models are implemented using an Artificial Neural Network with back propagation as the training algorithm. The network analyzes a time series and in return outputs the predicted value. The success of the network is much dependent on the number of layers, neurons and epochs and the answer is never unambiguous. It is necessary to experiment with different types of numbers in order to find the best match for the problem at hand.

\subsection{Layers and Neurons}
\label{sec:layersAndNeurons}
We are using a feedforward ANN which is often organized in one input layer, one or more hidden layers and an output layer where each layer has a number of neurons attached\cite{1} --- also see Figure~\ref{fig:ANN}. The number of input neurons is equal to the input parameters. The output layer contains neurons equal to the number of outputs. 
The complexity arises in the input and hidden layers because a lot of neuron/layer combinations exist. The number of hidden layers and its affiliated neurons is typically chosen by trial and error and then running simulations to find the best fit\cite{1}. The trial and error approach is because choosing the right number of neurons for the hidden layers is no exact science and we cannot calculate the numbers from a mathematical formula.

\subsection{Epochs}
\label{sec:epochs}
The network trains itself in a number of iterations also known as epochs. In every epoch the network is adjusting its weights based on an error measurement that indicates the difference between the desired output and the predicted output of the particular epoch\cite{1}. The goal is of course to adjust the weights in enough iterations so that this error measurement becomes as small as possible. This is not necessarily the same as training infinitely will result in the perfect error margin because the ANN can become overfitted as introduced in section~\ref{sec:annSection}. The purpose of the ANN is to make a generalization and not make perfect match for the training set. This ensures that it will be accurate on unseen data\cite{1} as well. The performance of the ANN is therefore to be measured by applying it to a new testing set that the network has never seen before.

\subsection{Pruning}
\label{sec:pruning}
The pruning of a network is a mechanism to ensure the best number of layers and neurons. Instead of doing a manual test of all the combinations we can use a pruning algorithm designed to do these steps for us. There are two different approaches an incremental approach and a selective approach\cite{stepniewski1997pruning}. The incremental \fnurl{pruning algorithm}{http://www.heatonresearch.com/wiki/NN_Session_9:_Pruning_Neural_Networks} takes a specific dataset and tests all the neuron and layer combinations in the range that you set it up to. This is a systematic approach and can easily be applied before starting any predictions. This method should be run before doing any calculations on an ANN to ensure the best possible network structure for the dataset at hand. Another approach is selective pruning. This kind of pruning takes an already trained network and analyzes it. The focus is to find the neurons from the hidden layers that have an insignificant role in the entire network thus not introducing a larger error margin if removed from the network. This allows us to get a smaller and more efficient network which should give us the most optimal solution. The difference between the two are the basis of what they are applied on. If we have no idea of what the best structure is before we begin then we want an incremental pruning. This is because the incremental pruning resembles trial and error and with no prior knowledge will give us the best network. This is a time consuming task but does have the benefit of finding the most optimal solution with no prior knowledge. The selective pruning on the other hand requires us to have an idea of what the best network will look like. From there it will cut away neurons that are not being used but it does not add new neurons to the network. Therefore we have to start out with a little larger network than what we expect to be the best for this algorithm to work.

We will be using incremental pruning because we have no prior knowledge about how the different inputs and formats of the inputs will affect the network structure thus making the incremental pruning the best choice for us. We will be using a built in pruning algorithm from the \fnurl{Encog Machine Learning Framework}{http://www.heatonresearch.com/encog}.