\relax 
\select@language{american}
\@writefile{toc}{\select@language{american}}
\@writefile{lof}{\select@language{american}}
\@writefile{lot}{\select@language{american}}
\@writefile{toc}{\contentsline {chapter}{Abstract}{iii}}
\@writefile{toc}{\contentsline {chapter}{Resum\'e}{v}}
\@writefile{toc}{\contentsline {chapter}{Acknowledgments}{vii}}
\citation{windPowerDanishLiberalized}
\citation{6}
\citation{windPowerDanishLiberalized}
\citation{pjmForecast}
\citation{21}
\citation{21}
\citation{FIND REF}
\citation{windPowerDanishLiberalized}
\citation{windPowerDanishLiberalized}
\citation{windPowerDanishLiberalized}
\citation{21}
\citation{FIND REF}
\citation{21}
\citation{21}
\citation{21}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:intro}{{1}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}The Problem}{3}}
\citation{1}
\citation{1}
\citation{2}
\citation{3}
\citation{5}
\citation{5}
\citation{8}
\citation{15}
\citation{17}
\citation{14}
\citation{17}
\citation{18}
\citation{1}
\citation{16}
\citation{2}
\citation{3}
\citation{17}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}The Method}{5}}
\citation{2}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}The hypothesis}{6}}
\citation{18}
\citation{18}
\citation{18}
\citation{18}
\citation{18}
\citation{18}
\citation{18}
\citation{18}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Related Work}{7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:related work}{{2}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Machine Learning}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The three components of learning algorithms \cite  {18}\relax }}{7}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:threeComponents}{{2.1}{7}}
\citation{18}
\citation{7}
\citation{9}
\citation{10}
\citation{8}
\citation{8}
\citation{13}
\citation{4}
\citation{5}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Example of bias and variance in a dart throw \cite  {18}\relax }}{8}}
\newlabel{fig:biasandvariance}{{2.2}{8}}
\citation{EnergyPriceForecasting}
\citation{ARIMA}
\citation{ARIMA}
\citation{1}
\citation{1}
\citation{1}
\citation{1}
\citation{stockForecasting}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Green energy prediction}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Electricity Prices}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Prediction}{9}}
\citation{stockForecasting}
\citation{pjmForecast}
\citation{pjmForecast}
\citation{pjmForecast}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Comparison between Neural Network and ARIMA in terms of error \cite  {1}\relax }}{10}}
\newlabel{fig:ArimaVSNN}{{2.3}{10}}
\citation{pjmForecast}
\citation{pjmForecast}
\citation{FIND A SOURCE}
\citation{18}
\citation{singhal2011electricity}
\citation{singhal2011electricity}
\citation{xie2006new}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Results of forecasting from \cite  {pjmForecast}.\relax }}{11}}
\newlabel{table:sdmresult}{{2.1}{11}}
\citation{chen2004load}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces The steps taken to create a Support Vector Machine\relax }}{12}}
\newlabel{fig:phasesOfSVM}{{2.4}{12}}
\citation{22}
\citation{19}
\citation{19}
\citation{20}
\citation{19}
\citation{19}
\citation{19}
\citation{19}
\citation{19}
\citation{19}
\citation{19}
\citation{19}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Applications}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Electricity Demand Prediction}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Shows ANN compared to B\&J and SE \cite  {19} \relax }}{14}}
\newlabel{fig:anncomparison}{{2.5}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Function of monthly CET as a function from 1970 to 1995 \cite  {19}\relax }}{14}}
\newlabel{fig:CET}{{2.6}{14}}
\citation{19}
\citation{19}
\citation{19}
\citation{19}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces The actual values and the predicted demand in a comparison \cite  {19}\relax }}{16}}
\newlabel{fig:predicteddemand}{{2.7}{16}}
\citation{buckland2002ai}
\citation{rojas1996neural}
\citation{stockForecasting}
\citation{stockForecasting}
\citation{21}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Neural Network}{17}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:neural network}{{3}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces A simple neural network with 3 layers. \cite  {stockForecasting}\relax }}{17}}
\newlabel{fig:weight_of_layers}{{3.1}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces How the weight is calculated from one layer to the next\relax }}{18}}
\newlabel{fig:weight_of_layers}{{3.2}{18}}
\citation{rojas1996neural}
\citation{rojas1996neural}
\citation{inductiveBias}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Unsupervised learning}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Supervised learning}{19}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces This training set is very simple yet it illustrates a training set for a supervised learning algorithm very well.\relax }}{19}}
\newlabel{table:nonlin}{{3.1}{19}}
\citation{meanSquaredError}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces \relax }}{20}}
\newlabel{fig:activationFunctions}{{3.3}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces \relax }}{20}}
\newlabel{fig:activationFunctionsWithBias}{{3.4}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Problems to be avoided}{20}}
\citation{buckland2002ai}
\citation{buckland2002ai}
\citation{buckland2002ai}
\citation{buckland2002ai}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces At the global minimum \cite  [P. 318]{buckland2002ai}\relax }}{21}}
\newlabel{fig:test1}{{3.5}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Stuck in a local minimum \cite  [P. 318]{buckland2002ai}\relax }}{21}}
\newlabel{fig:localMinimum}{{3.6}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces A. The plot graph of the input B. The generalized function C. An over-fit function\relax }}{21}}
\newlabel{fig:overfitting}{{3.7}{21}}
\bibstyle{plain}
\bibdata{refs}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Conclusion}{23}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:conclusion}{{4}{23}}
\@writefile{toc}{\contentsline {chapter}{Primary Bibliography}{23}}
\bibcite{2}{1}
\bibcite{5}{2}
\bibcite{EnergyPriceForecasting}{3}
\bibcite{buckland2002ai}{4}
\bibcite{3}{5}
\bibcite{20}{6}
\bibcite{1}{7}
\bibcite{chen2004load}{8}
\bibcite{ARIMA}{9}
\bibcite{6}{10}
\bibcite{18}{11}
\bibcite{22}{12}
\bibcite{13}{13}
\bibcite{19}{14}
\bibcite{pjmForecast}{15}
\bibcite{4}{16}
\bibcite{windPowerDanishLiberalized}{17}
\bibcite{17}{18}
\bibcite{21}{19}
\bibcite{9}{20}
\bibcite{7}{21}
\bibcite{15}{22}
\bibcite{rojas1996neural}{23}
\bibcite{singhal2011electricity}{24}
\bibcite{10}{25}
\bibcite{stockForecasting}{26}
\bibcite{14}{27}
\bibcite{inductiveBias}{28}
\bibcite{16}{29}
\bibcite{meanSquaredError}{30}
\bibcite{8}{31}
\bibcite{xie2006new}{32}
\@writefile{toc}{\contentsline {chapter}{Secondary Bibliography}{27}}
